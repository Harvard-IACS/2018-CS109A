{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <img style=\"float: left; padding-right: 10px; width: 35px\" src=\"https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/iacs.png\"> CS109A Introduction to Data Science\n",
    "\n",
    "## Standard Section 2: Prediction using kNN and Linear Regression\n",
    "\n",
    "\n",
    "**Harvard University**<br/>\n",
    "**Fall 2018**<br/>\n",
    "**Instructors:** Pavlos Protopapas, Kevin Rader<br/>\n",
    "**Section Leaders:** Cecilia Garraffo, Mehul Smriti Raje, Ken Arnold, Karan Motwani<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "blockquote { background: #AEDE94; }\n",
       "h1 { \n",
       "    padding-top: 25px;\n",
       "    padding-bottom: 25px;\n",
       "    text-align: left; \n",
       "    padding-left: 10px;\n",
       "    background-color: #DDDDDD; \n",
       "    color: black;\n",
       "}\n",
       "h2 { \n",
       "    padding-top: 10px;\n",
       "    padding-bottom: 10px;\n",
       "    text-align: left; \n",
       "    padding-left: 5px;\n",
       "    background-color: #EEEEEE; \n",
       "    color: black;\n",
       "}\n",
       "\n",
       "div.exercise {\n",
       "\tbackground-color: #ffcccc;\n",
       "\tborder-color: #E9967A; \t\n",
       "\tborder-left: 5px solid #800080; \n",
       "\tpadding: 0.5em;\n",
       "}\n",
       "div.theme {\n",
       "\tbackground-color: #DDDDDD;\n",
       "\tborder-color: #E9967A; \t\n",
       "\tborder-left: 5px solid #800080; \n",
       "\tpadding: 0.5em;\n",
       "\tfont-size: 18pt;\n",
       "}\n",
       "div.gc { \n",
       "\tbackground-color: #AEDE94;\n",
       "\tborder-color: #E9967A; \t \n",
       "\tborder-left: 5px solid #800080; \n",
       "\tpadding: 0.5em;\n",
       "\tfont-size: 12pt;\n",
       "}\n",
       "p.q1 { \n",
       "    padding-top: 5px;\n",
       "    padding-bottom: 5px;\n",
       "    text-align: left; \n",
       "    padding-left: 5px;\n",
       "    background-color: #EEEEEE; \n",
       "    color: black;\n",
       "}\n",
       "header {\n",
       "   padding-top: 35px;\n",
       "    padding-bottom: 35px;\n",
       "    text-align: left; \n",
       "    padding-left: 10px;\n",
       "    background-color: #DDDDDD; \n",
       "    color: black;\n",
       "}\n",
       "</style>\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#RUN THIS CELL \n",
    "import requests\n",
    "from IPython.core.display import HTML\n",
    "styles = requests.get(\"http://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/cs109.css\").text\n",
    "HTML(styles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this section, our goal is to get you familiarized with k-Nearest Neighbors and Linear. These methods find powerful applications in all walks of life and are centered around prediction. \n",
    "\n",
    "\n",
    "Specifically, we will: \n",
    "    \n",
    "    1. Review Basic Python Data Structures\n",
    "    2. Import Data and Manipulates Rows and Columns\n",
    "    3. Load in the Bikeshare dataset which is split into a training and testing dataset\n",
    "    3. Do some basic exploratory analysis of the dataset and go through a scatterplot\n",
    "    5. Write out the algorithm for kNN WITH AND WITHOUT using the sklearn package\n",
    "    6. Learn to use the sklearn package for Linear Regression.\n",
    "    7. What is and how to extract information about Confidence Intervals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this section we will be using the following packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Check Python Version\n",
    "import sys\n",
    "assert(sys.version_info.major==3),print(sys.version)\n",
    "\n",
    "#Matrices, Dataframe and Plotting Operations\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "#Model Packages for k-NN and Linear Regression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from statsmodels.api import OLS\n",
    "import statsmodels.api as sm\n",
    "\n",
    "#Metrics, Performance Evaluation and Helpful fucntions\n",
    "from sklearn import metrics, datasets\n",
    "from collections import Counter\n",
    "import statsmodels.api as sm\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Aesthetic settings\n",
    "from IPython.display import display\n",
    "pd.set_option('display.max_rows', 999)\n",
    "pd.set_option('display.width', 500)\n",
    "pd.set_option('display.notebook_repr_html', True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review – Python : List, Dictionary Comprehensions and Zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='exercise'> **Exercise**</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Makes a list of all the even numbers from 0 to 20 with a For Loop\n",
    "\n",
    "##Your Code Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load sol1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='exercise'> **Exercise**</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create a dictionary with the list A elements as keys and list B elements \n",
    "#as values using Zip and Dictionary Comprehension\n",
    "\n",
    "list_a = ['a','b','c','d','e']\n",
    "list_b = [1,2,3,4,5]\n",
    "\n",
    "##Your Code Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load sol2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with Dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Import Data\n",
    "data_path = 'data/states.csv'\n",
    "states = pd.read_csv(data_path, index_col=0)\n",
    "states.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data describing the population per state from 1900 to 2010"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select Specific Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='exercise'> **Exercise**</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Select by Name : Select only '1930','1940','1950','1960' columns.\n",
    "\n",
    "#Select by Index : Select only the first 3 columns\n",
    "\n",
    "#Select by List : Select the following list of columns\n",
    "columns_list = ['1920','1840']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load sol3.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learn to \"display\", always!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above statements, we use the display function which can be imported using ```from IPython.display import display```. It provides a very aesthetically pleasing representation of the data with great demarcation for row and column information. Let's see a comparison below :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(states[columns_list].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "display(states[columns_list].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select Specific Rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='exercise'> **Exercise**</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Filter out by First N Rows\n",
    "n = 25\n",
    "\n",
    "#Select by Index, rows 5 to 10.\n",
    "\n",
    "#Select by List of Indices below\n",
    "row_list = np.arange(1,25,2)\n",
    "print(row_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load sol4.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in the Bikeshare dataset and perform EDA:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The specific task is to build a regression model for a bike share system that can predict the total number of bike rentals in a given day, based on attributes about the day. Such a demand forecasting model would be useful in planning the number of bikes that need to be available in the system on any given day, and also in monitoring traffic in the city. The data for this problem was collected from the Capital Bikeshare program in Washington D.C. over two years.   \n",
    "\n",
    "The data set is provided in the file 'bikeshare.csv'. Each row in these files contains 10 attributes describing a day and its weather:\n",
    "- season (1 = spring, 2 = summer, 3 = fall, 4 = winter)\n",
    "- month (1 through 12, with 1 denoting Jan)\n",
    "- holiday (1 = the day is a holiday, 0 = otherwise)\n",
    "- day_of_week (0 through 6, with 0 denoting Sunday)\n",
    "- workingday (1 = the day is neither a holiday or weekend, 0 = otherwise)\n",
    "- weather \n",
    "    - 1: Clear, Few clouds, Partly cloudy, Partly cloudy\n",
    "    - 2: Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist\n",
    "    - 3: Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds\n",
    "    - 4: Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog \n",
    "- temp (temperature in Celsius)\n",
    "- atemp (apparent temperature, or relative outdoor temperature, in Celsius)\n",
    "- humidity (relative humidity)\n",
    "- windspeed (wind speed)\n",
    "\n",
    "and the last column 'count' contains the response variable, i.e. total number of bike rentals on the day.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Load the BikeShare dataset\n",
    "bikeshare = pd.read_csv('data/bikeshare.csv')\n",
    "del bikeshare['Unnamed: 0']\n",
    "print(\"Length of Dataset:\",len(bikeshare))\n",
    "display(bikeshare.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "display(bikeshare.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use the groupby function to look at mean stats aggregated by month: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bikeshare.groupby('month').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the variation of count with month? Is  there a seasonal change?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(bikeshare.groupby('month').mean()['count'])\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Bikeshare Rental Count as a function of Month')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is temp, a_temp, is there a difference?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(bikeshare['temp'], bikeshare['atemp'])\n",
    "plt.xlabel('Temp')\n",
    "plt.ylabel('A-Temp')\n",
    "plt.title('A-Temp vs Temp')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What did we do wrong here? Why does the plot look like this?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sorting! Whenever your plot makes zig-zag changes across the scale, it is because ```matplotlib``` is trying to connect the points sequentially from the top (using a line plot) and skipping across the scale when $x_{i+1}$ is lower than $x_{i}$. So let's sort."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new = bikeshare.sort_values(['temp'])\n",
    "plt.plot(new['temp'], new['atemp'])\n",
    "plt.xlabel('Temp')\n",
    "plt.ylabel('A-Temp')\n",
    "plt.title('A-Temp vs Temp')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**It still looks weird, why?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a closer look at the dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "display(new.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are multiple ```atemp``` values for each ```temp``` value, which if not sorted will bounce around at the same x-value. Thus, we need to sort both axes simultaneously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new = bikeshare.sort_values(['temp','atemp'])\n",
    "plt.plot(new['temp'], new['atemp'])\n",
    "plt.xlabel('Temp')\n",
    "plt.ylabel('A-Temp')\n",
    "plt.title('A-Temp vs Temp')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By plotting efficiently, we found an anomaly we would have otherwise overlooked. It looks like there is a problem with the data around ```temp greater than 30``` and ```atemp less than 10```."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='exercise'> **Exercise**</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load sol5.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anomaly! ```atemp``` and ```temp``` are usually lineary related except at this one datapoint. Now, we get to make a judgement call as to whether we should keep the datapoint? We'll come back to this question after the lecture on Missing Data and Imputation. Worth a thought though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bikeshare= bikeshare.drop([188])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "display(bikeshare[(bikeshare['temp']>30) & (bikeshare['atemp']<10)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, it is good practice to normalize data before proceeding. As such, we can create the following functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Function to takes in a dataset and normalizes it\n",
    "def normalize(x):\n",
    "    #print(x.shape)\n",
    "    #print(np.min(x).shape)\n",
    "    num = x - np.min(x)\n",
    "    denom = np.max(x) - np.min(x)\n",
    "    return (num / denom)\n",
    "\n",
    "#bikeshare_norm = normalize(bikeshare.iloc[:, 0:10])\n",
    "bikeshare_norm = normalize(bikeshare)\n",
    "display(bikeshare_norm.head())\n",
    "bikeshare_norm.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='exercise'> **Exercise**</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load sol6.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Adapt the normalization function above to take column names as input\n",
    "\n",
    "#Function to takes in a dataset and normalizes it\n",
    "def normalize_cols(x, columns):\n",
    "    for i in columns:  x[i] = normalize(x[i]) \n",
    "    return x\n",
    "\n",
    "bikeshare_norm2= normalize_cols(bikeshare ,bikeshare.columns )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "display(bikeshare_norm2.head())\n",
    "bikeshare.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what the describe function **AFTER** normalization:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Broadcasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image('http://www.scipy-lectures.org/_images/numpy_broadcasting.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "image from Scipy Lecture Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose one predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bikeshare.columns\n",
    "bikeshare.iloc[:,0:10].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#sns.pairplot(bikeshare, x_vars=bikeshare.iloc[:,:10] , y_vars =bikeshare['count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bikeshare = bikeshare[['temp', 'count']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load sol8.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bikeshare.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split up the data into a training set and a test set:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have an idea of what the data looks like, we would like to predict count. Therefore, we will be breaking up the data into a **training** and a **testing** set. The **training** set will be used to train the model, while the **testing** set will be used to gauge how well our model does in general. The **testing** set is a way for us to ensure our model doesn't overfit our training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us first create a function that will randomly split the data up into a 70-30 split, with 70% of the data going into the **testing** set:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='exercise'> **Exercise**</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Function to Split data into Train and Test Set, Enter Code Here\n",
    "def split_data(data):\n",
    "    \n",
    "    #Calculate Length of Dataset\n",
    "    \n",
    "    #Define Split\n",
    "    \n",
    "    #Set a random Seed For Shuffling\n",
    "    \n",
    "    #Generate a Mask with a X:Y Split\n",
    "    \n",
    "    #Separate train and test data using mask\n",
    "    \n",
    "    #Return Train and Test Data Separately\n",
    "    return data_train, data_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load sol6.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Split data using defined function\n",
    "train_data, test_data = split_data(bikeshare)\n",
    "print(\"Length of Training set:\",len(train_data))\n",
    "print(\"Length of Testing set:\",len(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='exercise'> **Exercise**</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Check that the ratio between test and train sets is right\n",
    "#Your Code Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load sol7.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Old vs new indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data.iloc[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data.loc[3:9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data.iloc[3:9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternative approach using Standard 'train_test_split' function from sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data, test_data = train_test_split(bikeshare, test_size=0.30, random_state=13)\n",
    "print(\"Length of Training set:\",len(train_data))\n",
    "print(\"Length of Testing set:\",len(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing the kNN Algorithm by hand:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To really understand how the kNN algorithm works, it helps to go through the algorithm line by line in code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#kNN Algorithm\n",
    "def knn_algorithm(train, test, k):\n",
    "    \n",
    "    #Create any empty list to store our predictions in\n",
    "    predictions = []\n",
    "    \n",
    "    #Separate the response and predictor variables from training and test set:\n",
    "    train_x = train['temp']\n",
    "    train_y = train['count']\n",
    "    test_x  = test['temp']\n",
    "    test_y  = test['count']\n",
    "    \n",
    "    for i, ele in enumerate(test_x):\n",
    "        \n",
    "        #For each test point, store the distance between all training points and test point\n",
    "        distances = pd.DataFrame((train_x.values - ele)**2 , index=train.index)\n",
    "        distances.columns =['dist']\n",
    "        \n",
    "        #display(distances)\n",
    "        #Then, we sum across the columns per row to obtain the Euclidean distance squared\n",
    "        ##distances = vec_distances.sum(axis = 1)\n",
    "        \n",
    "        #Sort the distances to training points (in ascending order) and take first k points\n",
    "        nearest_k = distances.sort_values(by='dist').iloc[:k]\n",
    "        \n",
    "        #For simplicity, we omitted the square rooting of the Euclidean distance because the\n",
    "        #square root function preserves order. \n",
    "        \n",
    "        #Take the mean of the y-values of training set corresponding to the nearest k points\n",
    "        k_mean = train_y[nearest_k.index].mean()\n",
    "        \n",
    "        #Add on the mean to our predicted y-value list\n",
    "        predictions.append(k_mean)\n",
    "    \n",
    " \n",
    "    \n",
    "    #Create a dataframe with the x-values from test and predicted y-values  \n",
    "    predict = test.copy()  \n",
    "    predict['predicted_count'] = pd.Series(predictions, index=test.index)\n",
    "    \n",
    "    return predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to run the algorithm on our dataset with $k = 5$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Run the kNN function \n",
    "\n",
    "k = 5\n",
    "predicted_knn = knn_algorithm(train_data, test_data, k)\n",
    "predicted_knn.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to have a way to evaluate our predictions from the kNN algorithm with $k=5$. One way is to compute the $R^2$ coefficient. Let's create a function for that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Test predictions in comparison to true value of test set\n",
    "def evaluate(predicted, true):\n",
    "    \n",
    "    #Find the squared error:\n",
    "    squared_error = (predicted['predicted_count'] - true['count'])**2\n",
    "    \n",
    "    #Finding the mean squared error:\n",
    "    error_var = squared_error.sum()\n",
    "    sample_var = ((true['count'] - true['count'].mean())**2).sum()\n",
    "    r = (1 - (error_var / sample_var))\n",
    "    return r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then let's apply this function to our predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Length of Test Data:\",len(test_data))\n",
    "print(\"R^2 Score of kNN - test:\", evaluate(predicted_knn, test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predicted_knn_train = knn_algorithm(train_data, train_data, k)\n",
    "\n",
    "print(\"R^2 Score of kNN - train:\", evaluate(predicted_knn_train, train_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the coefficient for the Nearest Neighbors implementation with $k=5$ is $R^2 = 0.134$, which should more or less match what we get with the sklearn package. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now using sklearn to implement kNN:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now use the **sklearn** package to implement kNN. Then, we can fit the model and use various metrics to assess our accuracy.\n",
    "\n",
    "**What is sklearn?**\n",
    "\n",
    "Scikit-learn provides a range of supervised and unsupervised learning algorithms via a consistent interface in Python.\n",
    "\n",
    "It is licensed under a permissive simplified BSD license and is distributed under many Linux distributions, encouraging academic and commercial use. The library is built upon the SciPy (Scientific Python) that must be installed before you can use scikit-learn. This stack that includes:\n",
    "\n",
    "* NumPy: Base n-dimensional array package\n",
    "* SciPy: Fundamental library for scientific computing\n",
    "* Matplotlib: Comprehensive 2D/3D plotting\n",
    "* IPython: Enhanced interactive console\n",
    "* Sympy: Symbolic mathematics\n",
    "* Pandas: Data structures and analysis\n",
    "* Extensions or modules for SciPy care conventionally named SciKits. As such, the module provides learning algorithms and is named scikit-learn.\n",
    "\n",
    "The vision for the library is a level of robustness and support required for use in production systems. This means a deep focus on concerns such as easy of use, code quality, collaboration, documentation and performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**General sklearn model fitting code-structure :**\n",
    "\n",
    "```\n",
    "#Split Data into Train and Test Set\n",
    "x_train, y_train = training_data.drop('Response_Variable', axis=1), training_data['Response_Variable']\n",
    "x_test, y_test = test_data.drop('Response_Variable', axis=1), test_data['Response_Variable']\n",
    "\n",
    "#Define Model\n",
    "model = sklearn_model_name(hyper_parameter1 = value1, hyper_parameter2 = value2)\n",
    "\n",
    "#Fit Model\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "#Get Prediction\n",
    "y_pred_train = model.predict(x_train)\n",
    "y_pred_test = model.predict(x_test)\n",
    "\n",
    "#Evaluate Model\n",
    "r2_train = model.score(y_train, y_pred_train)\n",
    "r2_test = model.score(y_test, y_pred_test)\n",
    "\n",
    "#Print Results\n",
    "print(\"Score for Model (Training):\", r2_train)\n",
    "print(\"Score for Model (Testing) :\", r2_test)\n",
    "```\n",
    "\n",
    "* Every model has a list of hyperparameters that can be set using sklearn for the specific problem. In practice it is advisable to cross-validate a list of values to find best model fit.\n",
    "\n",
    "* ```model.fit``` calculates the parameters of your model corresponding to the training data and hyperparameters you provided.\n",
    "\n",
    "* ```model.predict(X)``` is the standard method called to make the model predict values for a specific X. Depending on if you feed x_train or x_test, you will get a y_prediction_train or y_prediction_test respectively.\n",
    "\n",
    "* Evaluation of model can vary according to the task at hand i.e. Regression or Classification. For Regression, $R^2$ Score is standard while for Classification, Accuracy (%) is standard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set kNN parameter:\n",
    "k = 100\n",
    "\n",
    "# Now we can fit the model, predict our variable of interest, and then evaluate our fit:\n",
    "# First, we create the classifier object:\n",
    "neighbors = KNeighborsRegressor(n_neighbors=k)\n",
    "\n",
    "# Then, we fit the model using x_train as training data and y_train as target values:\n",
    "neighbors.fit(train_data[['temp']], train_data['count'])\n",
    "\n",
    "# Retreieve our predictions:\n",
    "prediction_knn = neighbors.predict(test_data[['temp']])\n",
    "\n",
    "# This returns the mean accuracy on the given test data and labels, or in other words, \n",
    "# the R squared value -- A constant model that always predicts the expected value of y, \n",
    "# disregarding the input features, would get a R^2 score of 1.\n",
    "r2_train = neighbors.score(train_data[['temp']], train_data['count'])\n",
    "r2_test = neighbors.score(test_data[['temp']], test_data['count'])\n",
    "print(\"Length of Test Data:\", len(test_data['count']))\n",
    "print(\"R^2 Score of kNN on test set:\", r_test)\n",
    "print(\"R^2 Score of kNN on training set:\", r_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# SubPlots\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(20,6))\n",
    "#axes[0].set_xlim([0.5, 0.7])\n",
    "axes[0].plot(train_data['temp'], train_data['count'], 'o', label = 'Data' )#, '*',  label='Predicted')\n",
    "axes[0].plot(train_data['temp'], neighbors.predict(train_data[['temp']]), '*', label = 'Prediction')\n",
    "axes[0].set_xlabel('Normalized Temperature')\n",
    "axes[0].set_ylabel('# of Rides')\n",
    "axes[0].set_title(\"Training Set\")\n",
    "axes[0].legend()\n",
    "\n",
    "axes[1].plot(test_data['temp'], test_data['count'],'o', label = 'Data' )#, '*')\n",
    "axes[1].plot(test_data['temp'], prediction_knn, '*', label = 'Prediction')\n",
    "axes[1].set_xlabel('Normalized Temperature')\n",
    "axes[1].set_ylabel('# of Rides')\n",
    "axes[1].set_title(\"Test Set\")\n",
    "axes[1].legend()\n",
    "\n",
    "fig.suptitle(\"Bike Rides\");\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='exercise'> **Exercise**</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Can you predict what will happen for k=1 and for k = number-of-observations-in-training-set? Try it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We just went over the kNN prediction method. Now, we will fit the same data, but Linear Regression. We will use a the same training/testing dataset as before and create our linear regression objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Split Data into X,Y\n",
    "x_train, y_train = train_data.drop(['count'],axis=1), train_data['count']\n",
    "x_test, y_test = test_data.drop(['count'],axis=1), test_data['count']\n",
    "\n",
    "#Add constant\n",
    "x_train_ca = sm.add_constant(x_train)\n",
    "x_test_ca = sm.add_constant(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "StatsModels use a Y followed by X structure while feeding data in contrast to sklearn that uses X followed by Y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#We must first create the linear regression object from stats model\n",
    "\n",
    "model = sm.OLS(y_train, x_train_ca)\n",
    "results = model.fit()\n",
    "print(results.params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will compute metrics that can be used to assess fit.\n",
    "\n",
    "**Note: sklearn.metrics is class of functions that consists of all the metrics we care about to evaluate our models. While it is not hard to implement them yourself, it is helpful to go through http://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#To compute the mean squared error (notice that we are now using the TEST set):\n",
    "print(\"R^2 Score for Linear Regression (Training):\", metrics.r2_score(y_train, results.predict(x_train_ca)))\n",
    "print(\"R^2 Score for Linear Regression (Testing) :\", metrics.r2_score(y_test, results.predict(x_test_ca)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Find the squared error:\n",
    "y_pred_train = results.predict(x_train_ca)\n",
    "squared_error_train = (y_pred_train - y_train)**2\n",
    " #Finding the mean squared error:\n",
    "error_var_train = squared_error_train.mean()\n",
    "\n",
    "sample_var_train = ((y_train - y_train.mean())**2).mean()\n",
    "\n",
    "\n",
    "\n",
    "y_pred_test = results.predict(x_test_ca)\n",
    "squared_error_test = (y_pred_test - y_test)**2\n",
    " #Finding the mean squared error:\n",
    "error_var_test = squared_error_test.mean()\n",
    "\n",
    "sample_var_test = ((y_test - y_test.mean())**2).mean()\n",
    "\n",
    "print(error_var_train, sample_var_train, 1 - error_var_train/sample_var_train)\n",
    "print(error_var_test, sample_var_test, 1 - error_var_test/sample_var_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(x_train, y_train, 'b+')\n",
    "plt.plot(x_test, y_test, 'r+')\n",
    "\n",
    "x_forpredict = np.linspace(0,1, 100)\n",
    "line_y = results.predict(sm.add_constant(x_forpredict))\n",
    "plt.plot(x_forpredict, line_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confidence Intervals\n",
    "\n",
    "In Data Science, a confidence interval (CI) is a type of interval estimate, computed from the statistics of the observed data, that might contain the true value of an unknown population parameter. Simply speaking, a Confidence Interval is a range of values we are fairly sure our true value lies in. \n",
    "\n",
    "It is important to remind ourselves here that Confidence Intervals belong to a parameter and not a statistic. Thus, they represent the window in which the true value exists for the entire population when all we have is a sample.\n",
    "\n",
    "#### Example : \n",
    "\n",
    "We measure the heights of 40 randomly chosen men, and get a mean height of 175cm, we also know the standard deviation of men's heights is 20cm. The 95% Confidence Interval is thus: 175cm ± 6.2cm. This says the true mean of ALL men (if we could measure all their heights) is likely to be between 168.8cm and 181.2cm.\n",
    "\n",
    "But it might not be! The \"95%\" says that 95% of experiments like we just did will include the true mean, but 5% won't.\n",
    "So there is a 1-in-20 chance (5%) that our Confidence Interval does NOT include the true mean.\n",
    "\n",
    "To get Confidence Intervals using StatsModels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Confidence Interval using Stats Model Summary\n",
    "thresh = 0.05\n",
    "intervals = results.conf_int(alpha=thresh)\n",
    "intervals = intervals.rename(index=str, columns={0:str(thresh/2*100)+\"%\",1:str((1-thresh/2)*100)+\"%\"})\n",
    "display(intervals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above block of code, ```results.conf_int(alpha=thresh)``` returns a dataframe with columns 0 and 1. We explained Confidence Intervals above where because we assume normal symetric distribution of data, the 95% Confidence Interval means there's 2.5% chance of the true value lying below the values in Column 0 and 2.5% chance of the true value lying above Column 1. \n",
    "\n",
    "Therefore, for a better understanding, given a constant threshold, you can rename the columns to suit the Confidence Interval values. The syntax is ```dataframe.rename(index=str, columns={old_col_name1:new_col_name1, old_col_name2:new_col_name2})```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_x.shape"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
